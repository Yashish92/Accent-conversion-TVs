<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Accent Conversion with Articulatory Representations">
  <meta name="keywords" content="Accent Conversion, PPG, TV, speech production">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Accent Conversion Audio Samples</title>

  <!-- Link to external fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <!-- Link to your downloaded static CSS files -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/custom.css"> <!-- Your custom CSS if any -->
  <!-- Favicon -->
  <link rel="icon" href="./static/images/favicon.svg">
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
        <span class="icon"><i class="fas fa-home"></i></span>
        Home
      </a>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Accent Conversion with Articulatory Representations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://yashish92.github.io/">Yashish M. Siriwardena</a><sup>1*</sup></span>
            <span class="author-block">Nathan Swedlow<sup>2</sup></span>
            <span class="author-block">Audrey Howard<sup>2</sup></span>
            <span class="author-block">Evan Gitterman<sup>2</sup></span>
            <span class="author-block">Dan Darcy<sup>2</sup></span>
            <span class="author-block">Carol Espy-Wilson<sup>1</sup></span>
            <span class="author-block">Andrea Fanelli<sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Maryland College Park</span>
            <span class="author-block"><sup>2</sup>Dolby Laboratories</span>
            <p style="font-size:17px;">* Work done during an internship at Dolby</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.05947"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.05947"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="todo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (coming soon)</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Yashish92/Accent-conversion-TVs/tree/gh-pages"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Test Samples</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/model_overview.jpg" width="719" height="245"
                 class="interpolation-image"
                 alt="model overview for accent conversion with reference."/>
      <h2 class="subtitle has-text-centered">
        Training and Accent conversion stages of the FAC pipeline.
      </h2>
    </div>
  </div>
</section>
<!--/ teaser image -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Conversion of non-native accented speech to native (American) English has a wide range of applications such as improving intelligibility of non-native speech. Previous work on this domain has used phonetic posteriograms as the target speech representation to train an acoustic model which is then used to extract a compact representation of input speech for accent conversion. In this work, we introduce the idea of using an effective articulatory speech representation, extracted from an acousticto-articulatory speech inversion system, to improve the acoustic model used in accent conversion. The idea to incorporate articulatory representations originates from their ability to well characterize accents in speech. To incorporate articulatory representations with conventional phonetic posteriograms, a multitask learning based acoustic model is proposed. Objective and subjective evaluations show that the use of articulatory representations can improve the effectiveness of accent conversion
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract -->

    <!-- TVs -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Articulatory Representations from Speech Inversion</h2>
        <div class="content has-text-justified">
          <p>
            Acoustic-to-articulatory speech inversion (SI) aims to infer articulatory dynamics from spoken sounds [1]. While efforts to interpret articulatory movements from continuous speech signals have a long history [2], they have typically been limited to tracking specific parts of the vocal tract, like the upper and lower lips, tongue tip, and velum closure. However, it’s essential not only to understand the primary effects of individual vocal tract movements, but also to grasp how these articulators interact. For instance, articulators such as the lips and jaw often cooperate to achieve specific vocal tract shapes [3]. Consequently, general SI systems prioritize understanding vocal tract constriction, estimating the degree and position of functional tract variables (TVs; from Articulatory Phonology in [3]), rather than solely focusing on individual articulator movement. During SI, acoustic features extracted from speech signals are used to predict these tract variables. This process involves learning an inverse mapping by training on a dataset containing matched acoustic and directly observed articulatory data.          </p>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="hero-body" style="display: flex; justify-content: center;">
        <img src="./static/images/tv_overview.jpg" width="719" height="245"
             class="interpolation-image"
             alt="model overview for accent conversion with reference."/>
        <h2 class="subtitle has-text-centered">
          Vocal tract variables and related articulators. The color shaded TVs are extracted from a speech inversion system
        </h2>
      </div>
    </div>
    <!--/ TVs -->

    <!-- AM model -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Acoustic Model</h2>
        <div class="content has-text-justified">
          <p>
            The proposed multi-task learning based acoustic model (AM) contains two bidirectional long short memory (BiLSTM) layers followed by up-sampling, dropout and fully connected (Linear) layers as shared model layers. A fully connected layer with softmax activation is used to estimate PPG outputs and a fully connected layer with tanh activation is used to estimate TV outputs. Figure 2 shows details of the model architecture implemented in PyTorch. Original train, dev and test splits (both -clean and -other) from the LibriSpeech dataset [4] was used to train all the acoustic model variants. All the audio files were first segmented to 2 second long segments and the shorter ones were zero padded at the end. As shown in figure 2, the acoustic model takes in HiddenUnit BERT (HuBERT) [5] speech embeddings extracted from the pre-trained HuBERT-large model as the input speech representation. The HuBERT speech embeddings are sampled at 50 Hz and have a dimensionality of 1024. Tri-phone Phonetic Posteriograms (PPGs) are extracted from a pretrained model [6] as one of the target speech representations. The extracted PPGs are sampled at 100Hz and have a dimensionality of 5816. As the other target speech representation, TVs are extracted from a pre-trained acoustic-to-articulatory speech inversion system. The TVs are sampled at 100 Hz and contain 6 distinct variables.  
          </p>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="hero-body" style="display: flex; justify-content: center;">
        <img src="./static/images/accent_conversion_AM.jpg" width="719" height="245"
             class="interpolation-image"
             alt="model overview for accent conversion with reference."/>
        <h2 class="subtitle has-text-centered">
          Proposed Multi-task learning based Acoustic model from which PPG only, TV only, and Combined BNFs are extracted for accent conversion.
        </h2>
      </div>
    </div>

    <!--/ AM model -->

    <!-- Audio Samples -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Audio Samples</h2>
        <div class="content">
          <h3 class="title is-4">PPG Only</h3>
          <audio controls>
            <source src="./static/audio/ppg_sample1.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
          </audio>
          <audio controls>
            <source src="./static/audio/ppg_sample2.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
          </audio>

          <h3 class="title is-4">TV Only</h3>
          <audio controls>
            <source src="./static/audio/tv_sample1.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
          </audio>
          <audio controls>
            <source src="./static/audio/tv_sample2.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
          </audio>

          <h3 class="title is-4">Combined</h3>
          <audio controls>
            <source src="./static/audio/combined_sample1.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
          </audio>
          <audio controls>
            <source src="./static/audio/combined_sample2.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>
    </div>
    <!--/ Audio Samples -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{author2024accent,
  author    = {Yashish M. Siriwardena, Nathan Swedlow, Audrey Howard, Evan Gitterman, Dan Darcy, Carol Espy-Wilson, Andrea Fanelli},
  title     = {Accent Conversion with Articulatory Representations},
  journal   = {INTERSPEECH},
  year      = {2024},
}</code></pre>
  </div>
</section>

<section class="section" id="References">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>
    <div class="ordlist">
      <ol>
        <li id="fn:im2wav" role="doc-endnote"><p>G. Sivaraman, V. Mitra, H. Nam, M. Tiede, and C. Espy-Wilson, “Unsupervised speaker adaptation for speaker independent acoustic to articulatory speech inversion,” The Journal of the Acoustical Society of America, vol. 146, no. 1, pp. 316–329, 2019. [Online]. Available: https://doi.org/10.1121/1.5116130 </p></li>
        <li id="fn:clipsonic" role="doc-endnote"><p>G. Papcun, J. Hochberg, T. R. Thomas, F. Laroche, J. Zacks, and S. Levy, “Inferring articulation and recognizing gestures from acoustics with a neural network trained on x-ray microbeam data.” The Journal of the Acoustical Society of America, vol. 92, no. 2 Pt 1, pp. 688–700, aug 1992. [Online]. Available: http://www.ncbi.nlm.nih.gov/pubmed/1506525</p></li>
        <li id="fn:makeanaudio" role="doc-endnote"><p>C. P. Browman and L. Goldstein, “Articulatory Phonology : An Overview *,” Phonetica, vol. 49, pp. 155–180, 1992</p></li>
        <li id="fn:vggsound" role="doc-endnote"><p>V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An asr corpus based on public domain audio books,” 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p></li>
        <li id="fn:vggsound" role="doc-endnote"><p>V. W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 29</p></li>
        <li id="fn:vggsound" role="doc-endnote"><p>G. Zhao, S. Sonsaat, J. Levis, E. Chukharev-Hudilainen, and R. Gutierrez-Osuna, “Accent conversion using phonetic posteriorgrams,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p></li>
      </ol>
    </div>
    
    
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://example.com/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/example" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<!-- Link to your downloaded static JavaScript files -->
<script src="./static/js/jquery.min.js"></script>
<script src="./static/js/custom.js"></script>
<script>
  // Activate the navbar burger menu on click
  document.addEventListener('DOMContentLoaded', () => {
    const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
    if ($navbarBurgers.length > 0) {
      $navbarBurgers.forEach(el => {
        el.addEventListener('click', () => {
          const target = el.dataset.target;
          const $target = document.getElementById(target);
          el.classList.toggle('is-active');
          $target.classList.toggle('is-active');
        });
      });
    }
  });
</script>

</body>
</html>
